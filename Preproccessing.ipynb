{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Data & Save them on Google Drive\n",
        "we are using Persian commonvoice mozilla dataset from  'https://commonvoice.mozilla.org/fa/datasets'"
      ],
      "metadata": {
        "id": "m7V50iOGh3rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests \n",
        "file_url = \"https://mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com/cv-corpus-10.0-2022-07-04/cv-corpus-10.0-2022-07-04-fa.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQ3GQRTO3JABAMQFK%2F20220719%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220719T041459Z&X-Amz-Expires=43200&X-Amz-Security-Token=FwoGZXIvYXdzENX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDEno8%2B1bC0LzztdnfCKSBDCX6IPSfaygIDeWPEbiAmX%2Ft8S1IFKCNd1tobHrJVpmve0KOsrjGPQf266fPZdsyyyoMfyj6d2qVH1HJqoQ29n6fZnXUmADL%2FWxRj1mrhHjvFJ8nlYKgLVWSJJoOXwD9jh4Hb%2FobGZg8If6s0sMulVNPKGEsA5LRVzZ4lHRQFNpYybuC6cjSYSM6%2Bpfa2ckZqgdPsfw%2BO11OtKGTeVOJIMzC6%2BoOX%2FKPBmhHbyRR%2BRH3Kw1NdTmYXVIVHQWNt%2BtMtxnFsG82tvD9L%2B4y7kZgM686yIOE6aINXnxY%2BhOe%2F1%2FrJnAJggVOxgT0AgD%2Becl5iggrMowTKGTBpi5tNoCUdwB5OGTsCqPRJhenxpak%2BTB1mQMoeEtMVISVIZlJXwyyB78EIhyka0bepMVfrE26R6EX1lcITyirANbVEiRW7RUffjdS8XGUFbQrnZ3XWA9zfvo8XO6m%2BAx7%2B7UVswj5a7TfXudHqWu7bkRNaWp%2FJdtJLLrtjud1OBZwt4JjraWI%2F2Hv52hk6kzfg9OrZ1raZJuz3jqGdmf%2BJ2Izrtyl2%2FXbdB3ER1a1RjOIyAgC77Y5izJ5jg3VDhNps0hGSWVId3yvmj8g6F9R6B35ToXwV1seQoCK%2BLY12%2BBIqMVwsHtQPaQgHgMNU21V7EDy4SYj118PY2A32C0%2BbnFvhkhyqZ70Cbwkv0RBq%2FiaCh70P1WZN2wKO%2FO2JYGMioSa90REtd1agBMleue3lv5O%2FcIhPD%2BvQPtN0lQnRS2EX4cp%2Fjp7uSnkFQ%3D&X-Amz-Signature=3275cd41a1114231c0153e356595276f30823902d36dd282e2779c80f51ff348&X-Amz-SignedHeaders=host\"\n",
        "    \n",
        "r = requests.get(file_url, stream = True) \n",
        "  \n",
        "with open(\"/content/drive/MyDrive/cv_corpus-10.0_2022-07-04-fa.tar.gz\", \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block: \n",
        "             file.write(block) \n",
        "\n",
        "\n",
        "! tar -xf /content/drive/MyDrive/cv_corpus-10.0_2022-07-04-fa.tar.gz"
      ],
      "metadata": {
        "id": "O5IYaXKyioDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we save test, train, dev files and transcripts and get rid of useless files"
      ],
      "metadata": {
        "id": "PGXCq4G-jlnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning\n"
      ],
      "metadata": {
        "id": "FeX5D57Fj3VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_ds = pd.read_csv(\"/content/drive/MyDrive/train.tsv\", delimiter=\"\\t\")\n",
        "validation_ds = pd.read_csv(\"/content/drive/MyDrive/dev.tsv\", delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "GYTiRWbEhWRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "showcasing a sample file"
      ],
      "metadata": {
        "id": "_MLEd5prXxao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_sample, sample_rate = librosa.load(\"/content/train_files/common_voice_fa_24874520.wav\")\n",
        "display.display(display.Audio(audio_sample, rate=sample_rate))\n",
        "print(sample_rate)"
      ],
      "metadata": {
        "id": "aH5MGfsRXali"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting from mp3 to wav"
      ],
      "metadata": {
        "id": "M5EU2jHSXtCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "source_dir = \"/content/cv-corpus-10.0-2022-07-04/fa/clips\"\n",
        "for file_name in  os.listdir(source_dir):\n",
        "  train_path = \"/content/train_files\"\n",
        "  test_path = \"/content/test_files\"\n",
        "  dev_path = \"/content/dev_files\"\n",
        "  if file_name in train_ds.path.to_numpy():\n",
        "    file_path = os.path.join(train_path, file_name)\n",
        "\n",
        "    sound = AudioSegment.from_mp3(os.path.join(source_dir, file_name))\n",
        "    sound.export(file_path.split(\".\")[0] + \".wav\", format=\"wav\")\n",
        "\n",
        "  elif file_name in test_ds.path.to_numpy():\n",
        "    file_path = os.path.join(test_path, file_name)\n",
        "    \n",
        "    sound = AudioSegment.from_mp3(os.path.join(source_dir, file_name))\n",
        "    sound.export(file_path.split(\".\")[0] + \".wav\", format=\"wav\")\n",
        "\n",
        "  elif file_name in validation_ds.path.to_numpy():\n",
        "    file_path = os.path.join(dev_path, file_name)\n",
        "    sound = AudioSegment.from_mp3(os.path.join(source_dir, file_name))\n",
        "    sound.export(file_path.split(\".\")[0] + \".wav\", format=\"wav\")\n",
        "  "
      ],
      "metadata": {
        "id": "uB8roMv5Xs2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing some of the more unique ones, to check how they sounf"
      ],
      "metadata": {
        "id": "VjwV474ZX_X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_audio_1 = train_ds[train_ds.sentence == \"پای آهنگ آلبوم \\\"The Little Tin Soldier\\\" را برای پشت صحنه انتخاب کرد.\"].path.iloc[0].replace(\".mp3\", \".wav\")\n",
        "display.display(display.Audio((f\"/content/drive/MyDrive/train_wav/{sample_audio_1}\"), rate=sample_rate))"
      ],
      "metadata": {
        "id": "nBNX47v-X_Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_audio_2 = train_ds[train_ds.sentence == \"بیست = xx\"].path.iloc[0].replace(\".mp3\", \".wav\")\n",
        "display.display(display.Audio((f\"/content/drive/MyDrive/train_wav/{sample_audio_2}\"), rate=sample_rate))"
      ],
      "metadata": {
        "id": "SUrK6GocYIwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we extract the vocab from dev and train transcripts, and we try to clean it"
      ],
      "metadata": {
        "id": "uXrneMjGj_Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "all_sentences = np.concatenate((train_sents, val_sents),axis=0)\n",
        "all_sentences = ' '.join(all_sentences)\n",
        "\n",
        "sentences_tokens = Tokenizer(char_level=True)\n",
        "sentences_tokens.fit_on_texts([all_sentences])\n",
        "\n",
        "sentences_tokens.word_index.keys()"
      ],
      "metadata": {
        "id": "fKyaIIoMgrj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clearing out vocab from bad characters"
      ],
      "metadata": {
        "id": "WAoR9kfVkIn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_type_swap ={\n",
        "          ord(\"ى\")   :ord(\"ی\"),\n",
        "          ord(\"ﯽ\")   : ord(\"ی\"),\n",
        "          ord(\"ﺩ\")    :ord(\"د\"),\n",
        "          ord(\"ﻮ\")     :ord(\"و\"),\n",
        "          ord(\"ﻢ\")     :ord(\"م\"),\n",
        "          ord(\"ﺍ\")     :ord(\"ا\"),\n",
        "          ord(\"ﺱ\")    :ord(\"س\"),\n",
        "          ord(\"ﻡ\")    :ord(\"م\"),\n",
        "          ord('ﺘ')    : ord(\"ت\"),\n",
        "          ord('ە')    : ord(\"ه\"),\n",
        "          ord('ﮐ')    : ord(\"ک\"),\n",
        "          ord('ﺧ')   : ord(\"خ\"),\n",
        "          ord('ﭘ')    : ord(\"پ\"),\n",
        "          ord('ﺑ')    : ord(\"ب\"),\n",
        "          ord('ﻀ')   : ord(\"ض\"),\n",
        "          ord('ﻌ')    : ord(\"ع\"),\n",
        "          ord('ﺧ')    : ord(\"خ\"),\n",
        "          ord('ﺘ')    : ord(\"ت\"),\n",
        "          ord('ﻪ')    : ord(\"ه\"),\n",
        "          ord('ﻟ')    : ord(\"ل\"),\n",
        "          ord(\"ﺎ\")    : ord(\"ا\")\n",
        "}\n",
        "arabic_to_persian = {\n",
        "          ord('ة')  : ord('ه'),\n",
        "\t\t\t    ord('ك')  : ord('ک'),\n",
        "          ord('ي')  : ord('ی'),\n",
        "          ord('ئ')  : ord('ی'),\n",
        "\t\t\t    ord('أ')   : ord('ا'),\n",
        "          ord('ۀ')   : ord('ه'),\n",
        "          ord('ؤ')   : ord('و'),\n",
        "          ord('ے')  : ord('ی'),\n",
        "}\n",
        "special_chars_clear = {\n",
        "          ord('\"')     : None,\n",
        "          ord('\\'')    : None,\n",
        "          ord('ـ')     : None,\n",
        "          ord('َ')      : None,\n",
        "          ord('ُ')      : None,\n",
        "          ord('ِ')      : None,\n",
        "          ord('ّ')      : None,\n",
        "          ord('š')    : None,\n",
        "          ord('ā')    : None,\n",
        "          ord(':')    : None,\n",
        "          ord('٬')    : None,\n",
        "          ord('«')    : None,\n",
        "          ord('»')    : None,\n",
        "          ord(')')    : None,\n",
        "          ord('(')    :None,\n",
        "          ord(';')    :None,\n",
        "          ord('٬')    :None,\n",
        "          ord('؛')    :None,\n",
        "          ord(':')   :None,\n",
        "          ord('”')     :None,\n",
        "          ord('_')     :None,\n",
        "          ord('…')     :None,\n",
        "          ord('،')     :None,\n",
        "          ord('ء')     :None,\n",
        "          ord('”')     :None,\n",
        "          ord('_')     :None,\n",
        "          ord('…')     :None,\n",
        "          ord('ء')     :None,\n",
        "          ord('؟')     :None,\n",
        "          ord('!')     :None,\n",
        "          ord('،')     :None,\n",
        "          ord('“')     :None,\n",
        "          ord('ْ')       :None,\n",
        "          ord(',')     :None,\n",
        "          ord('\\t')     :None,\n",
        "          ord('\\n')     :None,\n",
        "          ord('-')     :None,\n",
        "          ord('ٔ')      :None,\n",
        "          ord('–')    :None\n",
        "    }    \n",
        "\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.translate(arabic_to_persian))\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.translate(special_chars_clear))\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.translate(char_type_swap))\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.replace(\"=\", \"مساوی\"))\n",
        "\n",
        "\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.translate(arabic_to_persian))\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.translate(special_chars_clear))\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.translate(char_type_swap))\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.replace(\"=\", \"مساوی\"))"
      ],
      "metadata": {
        "id": "z9LVw9yohK_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since out data is big we use half of our datas for training"
      ],
      "metadata": {
        "id": "SgayaprHk3zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_ds = pd.read_csv(\"/content/drive/MyDrive/train.tsv\", delimiter=\"\\t\")\n",
        "validation_ds = pd.read_csv(\"/content/drive/MyDrive/dev.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "train_ds = train_ds.sample(int(train_ds.shape[0]/2))\n",
        "validation_ds = validation_ds.sample(int(validation_ds.shape[0]/2))"
      ],
      "metadata": {
        "id": "zUJhwZ93grc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating data manifests as required by nemo models"
      ],
      "metadata": {
        "id": "R2OELrxjlFF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import librosa\n",
        "\n",
        "train_manifest = '/content/drive/MyDrive/train_manifest.json'\n",
        "test_manifest = '/content/drive/MyDrive/test_manifest.json'\n",
        "dev_manifest =  '/content/drive/MyDrive/dev_manifest.json'\n",
        "\n",
        "# Function to build a manifest\n",
        "def build_manifest(meta_data, dir_path, manifest_path):\n",
        "        with open(manifest_path, 'w') as fout:\n",
        "            for file_id in meta_data.index.to_numpy():\n",
        "                file_name = meta_data[meta_data.index == file_id].path.to_numpy()[0].split(\".\")[0]\n",
        "                transcript = meta_data[meta_data.index == file_id].sentence.to_numpy()[0]\n",
        "                audio_path = os.path.join(dir_path, file_name + \".wav\")\n",
        "                duration = librosa.core.get_duration(filename=audio_path)\n",
        "               \n",
        "                # Write the metadata to the manifest\n",
        "                metadata = {\n",
        "                    \"audio_filepath\": audio_path,\n",
        "                    \"duration\": duration,\n",
        "                    \"text\": transcript\n",
        "                }\n",
        "                json.dump(metadata, fout)\n",
        "                fout.write('\\n')\n",
        "                \n",
        "build_manifest(train_ds, \"/content/drive/MyDrive/train_wav\", train_manifest)\n",
        "build_manifest(test_ds, \"/content/drive/MyDrive/test_wav\", test_manifest)\n",
        "build_manifest(validation_ds, \"/content/drive/MyDrive/dev_wav\", dev_manifest)"
      ],
      "metadata": {
        "id": "YryfI9dVgrfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preproccessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}