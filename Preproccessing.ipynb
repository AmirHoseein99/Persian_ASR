{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Data & Save them on Google Drive\n",
        "we are using Persian commonvoice mozilla dataset from  'https://commonvoice.mozilla.org/fa/datasets'"
      ],
      "metadata": {
        "id": "m7V50iOGh3rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_link = \n",
        "! wget -O dataset_file.zip dataset_link\n",
        "! unzip dataset_file"
      ],
      "metadata": {
        "id": "O5IYaXKyioDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we save test, train, dev files and transcripts and get rid of useless files"
      ],
      "metadata": {
        "id": "PGXCq4G-jlnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning\n"
      ],
      "metadata": {
        "id": "FeX5D57Fj3VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_ds = pd.read_csv(\"/content/drive/MyDrive/train.tsv\", delimiter=\"\\t\")\n",
        "validation_ds = pd.read_csv(\"/content/drive/MyDrive/dev.tsv\", delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "GYTiRWbEhWRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we extract the vocab from dev and train transcripts, and we try to clean it"
      ],
      "metadata": {
        "id": "uXrneMjGj_Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "all_sentences = np.concatenate((train_sents, val_sents),axis=0)\n",
        "all_sentences = ' '.join(all_sentences)\n",
        "\n",
        "sentences_tokens = Tokenizer(char_level=True)\n",
        "sentences_tokens.fit_on_texts([all_sentences])\n",
        "\n",
        "sentences_tokens.word_index.keys()"
      ],
      "metadata": {
        "id": "fKyaIIoMgrj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clearing out vocab from bad characters"
      ],
      "metadata": {
        "id": "WAoR9kfVkIn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_type_swap ={\n",
        "          ord(\"ى\")   :ord(\"ی\"),\n",
        "          ord(\"ﯽ\")   : ord(\"ی\"),\n",
        "          ord(\"ﺩ\")    :ord(\"د\"),\n",
        "          ord(\"ﻮ\")     :ord(\"و\"),\n",
        "          ord(\"ﻢ\")     :ord(\"م\"),\n",
        "          ord(\"ﺍ\")     :ord(\"ا\"),\n",
        "          ord(\"ﺱ\")    :ord(\"س\"),\n",
        "          ord(\"ﻡ\")    :ord(\"م\"),\n",
        "          ord('ﺘ')    : ord(\"ت\"),\n",
        "          ord('ە')    : ord(\"ه\"),\n",
        "          ord('ﮐ')    : ord(\"ک\"),\n",
        "          ord('ﺧ')   : ord(\"خ\"),\n",
        "          ord('ﭘ')    : ord(\"پ\"),\n",
        "          ord('ﺑ')    : ord(\"ب\"),\n",
        "          ord('ﻀ')   : ord(\"ض\"),\n",
        "          ord('ﻌ')    : ord(\"ع\"),\n",
        "          ord('ﺧ')    : ord(\"خ\"),\n",
        "          ord('ﺘ')    : ord(\"ت\"),\n",
        "          ord('ﻪ')    : ord(\"ه\"),\n",
        "          ord('ﻟ')    : ord(\"ل\"),\n",
        "          ord(\"ﺎ\")    : ord(\"ا\")\n",
        "}\n",
        "arabic_to_persian = {\n",
        "          ord('ة')  : ord('ه'),\n",
        "\t\t\t    ord('ك')  : ord('ک'),\n",
        "          ord('ي')  : ord('ی'),\n",
        "          ord('ئ')  : ord('ی'),\n",
        "\t\t\t    ord('أ')   : ord('ا'),\n",
        "          ord('ۀ')   : ord('ه'),\n",
        "          ord('ؤ')   : ord('و'),\n",
        "          ord('ے')  : ord('ی'),\n",
        "}\n",
        "special_chars_clear = {\n",
        "          ord('\"')     : None,\n",
        "          ord('\\'')    : None,\n",
        "          ord('ـ')     : None,\n",
        "          ord('َ')      : None,\n",
        "          ord('ُ')      : None,\n",
        "          ord('ِ')      : None,\n",
        "          ord('ّ')      : None,\n",
        "          ord('š')    : None,\n",
        "          ord('ā')    : None,\n",
        "          ord(':')    : None,\n",
        "          ord('٬')    : None,\n",
        "          ord('«')    : None,\n",
        "          ord('»')    : None,\n",
        "          ord(')')    : None,\n",
        "          ord('(')    :None,\n",
        "          ord(';')    :None,\n",
        "          ord('٬')    :None,\n",
        "          ord('؛')    :None,\n",
        "          ord(':')   :None,\n",
        "          ord('”')     :None,\n",
        "          ord('_')     :None,\n",
        "          ord('…')     :None,\n",
        "          ord('،')     :None,\n",
        "          ord('ء')     :None,\n",
        "          ord('”')     :None,\n",
        "          ord('_')     :None,\n",
        "          ord('…')     :None,\n",
        "          ord('ء')     :None,\n",
        "          ord('؟')     :None,\n",
        "          ord('!')     :None,\n",
        "          ord('،')     :None,\n",
        "          ord('“')     :None,\n",
        "          ord('ْ')       :None,\n",
        "          ord(',')     :None,\n",
        "          ord('\\t')     :None,\n",
        "          ord('\\n')     :None,\n",
        "          ord('-')     :None,\n",
        "          ord('ٔ')      :None,\n",
        "          ord('–')    :None\n",
        "    }    \n",
        "\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.translate(arabic_to_persian))\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.translate(special_chars_clear))\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.translate(char_type_swap))\n",
        "train_ds[\"sentence\"] = train_ds.sentence.apply(lambda x : x.replace(\"=\", \"مساوی\"))\n",
        "\n",
        "\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.translate(arabic_to_persian))\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.translate(special_chars_clear))\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.translate(char_type_swap))\n",
        "validation_ds[\"sentence\"] = validation_ds.sentence.apply(lambda x : x.replace(\"=\", \"مساوی\"))"
      ],
      "metadata": {
        "id": "z9LVw9yohK_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since out data is big we use half of our datas for training"
      ],
      "metadata": {
        "id": "SgayaprHk3zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_ds = pd.read_csv(\"/content/drive/MyDrive/train.tsv\", delimiter=\"\\t\")\n",
        "validation_ds = pd.read_csv(\"/content/drive/MyDrive/dev.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "train_ds = train_ds.sample(int(train_ds.shape[0]/2))\n",
        "validation_ds = validation_ds.sample(int(validation_ds.shape[0]/2))"
      ],
      "metadata": {
        "id": "zUJhwZ93grc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating data manifests as required by nemo models"
      ],
      "metadata": {
        "id": "R2OELrxjlFF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import librosa\n",
        "\n",
        "train_manifest = '/content/drive/MyDrive/train_manifest.json'\n",
        "test_manifest = '/content/drive/MyDrive/test_manifest.json'\n",
        "dev_manifest =  '/content/drive/MyDrive/dev_manifest.json'\n",
        "\n",
        "# Function to build a manifest\n",
        "def build_manifest(meta_data, dir_path, manifest_path):\n",
        "        with open(manifest_path, 'w') as fout:\n",
        "            for file_id in meta_data.index.to_numpy():\n",
        "                file_name = meta_data[meta_data.index == file_id].path.to_numpy()[0].split(\".\")[0]\n",
        "                transcript = meta_data[meta_data.index == file_id].sentence.to_numpy()[0]\n",
        "                audio_path = os.path.join(dir_path, file_name + \".wav\")\n",
        "                duration = librosa.core.get_duration(filename=audio_path)\n",
        "               \n",
        "                # Write the metadata to the manifest\n",
        "                metadata = {\n",
        "                    \"audio_filepath\": audio_path,\n",
        "                    \"duration\": duration,\n",
        "                    \"text\": transcript\n",
        "                }\n",
        "                json.dump(metadata, fout)\n",
        "                fout.write('\\n')\n",
        "                \n",
        "build_manifest(train_ds, \"/content/drive/MyDrive/train_wav\", train_manifest)\n",
        "build_manifest(test_ds, \"/content/drive/MyDrive/test_wav\", test_manifest)\n",
        "build_manifest(validation_ds, \"/content/drive/MyDrive/dev_wav\", dev_manifest)"
      ],
      "metadata": {
        "id": "YryfI9dVgrfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preproccessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}